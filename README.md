# Library

### Deep Learning: 

- [LCA: Loss Change Allocation for Neural Network Training](https://arxiv.org/abs/1909.01440)
- [Asymptotics of Wide Networks from Feynman Diagrams](https://arxiv.org/abs/1909.11304)
- [Neural networks and physical systems with emergent collective computational abilities](https://www.pnas.org/content/79/8/2554)
- [Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes](https://arxiv.org/abs/1903.08778)
- [Adversarial Robustness Through Local Lipschitzness](https://arxiv.org/abs/2003.02460)
- [Lagrangian Neural Networks](https://arxiv.org/abs/2003.04630)

#### Mean Field Theory/ EOC/ Dynamic Isometry:

- [Deep Information Propagation](https://arxiv.org/abs/1611.01232)
- [Exponential expressivity in deep neural networks through transient chaos](https://arxiv.org/abs/1606.05340)
- [Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice](https://arxiv.org/abs/1711.04735)
- [Dynamical Isometry is Achieved in Residual Networks in a Universal Way for any Activation Function](https://arxiv.org/pdf/1809.08848v3.pdf)
- [Mean Field Residual Networks: On the Edge of Chaos](https://arxiv.org/abs/1712.08969)
- [Mean Field Theory of Activation Functions in Deep Neural Networks](https://arxiv.org/abs/1805.08786)
- [Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks](https://arxiv.org/abs/1806.05393)
- [On the Impact of the Activation Function on Deep Neural Networks Training](https://arxiv.org/abs/1902.06853)
- [Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks](https://arxiv.org/abs/1806.05394)

#### Optimization/ Line Search/ Wolfe's Theorem:

- [Convergence guarantees for RMSProp and ADAM in non-convex optimization and an empirical comparison to Nesterov acceleration](https://arxiv.org/pdf/1807.06766.pdf)
- [Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence](https://arxiv.org/pdf/2002.10542.pdf)
- [Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates](https://arxiv.org/pdf/1905.09997.pdf)
- [Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak- Lojasiewicz Condition](https://arxiv.org/pdf/1608.04636.pdf)
- [On the distance between two neural networks and the stability of learning](https://arxiv.org/abs/2002.03432)
- [The large learning rate phase of deep learning: the catapult mechanism](https://arxiv.org/pdf/2003.02218.pdf)
- [Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates](https://papers.nips.cc/paper/8630-painless-stochastic-gradient-interpolation-line-search-and-convergence-rates.pdf)
- [A Fine-Grained Spectral Perspective on Neural Networks](https://arxiv.org/abs/1907.10599)

#### Non-Linear Dynamics: 

- [Regularizing activations in neural networks via distribution matching with the Wasserstein metric](https://openreview.net/pdf?id=rygwLgrYPB)
- [Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem](https://arxiv.org/abs/1912.04378)
- [Effect of Activation Functions on the Training of Overparametrized Neural Nets](https://arxiv.org/abs/1908.05660v4)
- [Implicit Neural Representations with Periodic Activation Functions](https://arxiv.org/abs/2006.09661)
- [Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem](https://arxiv.org/pdf/1812.05720.pdf)

### Computer Vision:

- [Making Convolutional Networks Shift-Invariant Again](https://arxiv.org/abs/1904.11486)
- [GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing](https://arxiv.org/abs/1908.03245)
- [Butterfly Transform: An Efficient FFT Based Neural Architecture Design](https://arxiv.org/abs/1906.02256)
- [ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network](https://arxiv.org/pdf/2007.00992.pdf)
- [Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains](https://arxiv.org/abs/2006.10739)
- [Learning One Convolutional Layer with Overlapping Patches](https://arxiv.org/abs/1802.02547)
- [Batch-Shaping for Learning Conditional Channel Gated Networks](https://arxiv.org/abs/1907.06627)
- [Convolutional Networks with Adaptive Inference Graphs](https://arxiv.org/abs/1711.11503)

### Incremental Learning/ Continual Learning/ Lifelong Learning:

- [Conditional Channel Gated Networks for Task-Aware Continual Learning](https://arxiv.org/abs/2004.00070)
- [Supermasks in Superposition](https://arxiv.org/pdf/2006.14769.pdf)

### Mathematics (Mostly Abstract Algebra/ Topology/ Statistical Mechanics): 

- [Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning](https://www.cis.upenn.edu/~jean/math-deep.pdf)
- [ALGEBRA](https://solisinvicti.com/books/TheOlympiad/Books/AlgebraArtin.pdf)
- [Contemporary Abstract Algebra](https://people.clas.ufl.edu/cmcyr/files/Abstract-Algebra-Text_Gallian-e8.pdf)
- [Statistical Mechanics of Deep Learning](https://www.annualreviews.org/doi/full/10.1146/annurev-conmatphys-031119-050745)
